# Generated via helm template helm/llm-d-inference-sim in llm-d/llm-d-inference-sim.
# Source: llm-d-inference-sim/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-d-inference-sim-config
  labels:
    helm.sh/chart: llm-d-inference-sim-0.1.0
    app.kubernetes.io/name: llm-d-inference-sim
    app.kubernetes.io/instance: llm-d-inference-sim
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    port: 8001
    model: "Qwen/Qwen2-0.5B"
    served-model-name:
    - model1
    - model2
    max-loras: 2
    max-cpu-loras: 5
    max-num-seqs: 1000
    max-model-len: 32768
    lora-modules:
    - '{"name":"lora1","path":"/path/to/lora1"}'
    - '{"name":"lora2","path":"/path/to/lora2"}'
    mode: "echo"
    time-to-first-token: 5000
    inter-token-latency: 500
    kv-cache-transfer-latency: 50
    seed: 1.001001e+08
---
# Source: llm-d-inference-sim/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: llm-d-inference-sim
  labels:
    helm.sh/chart: llm-d-inference-sim-0.1.0
    app.kubernetes.io/name: llm-d-inference-sim
    app.kubernetes.io/instance: llm-d-inference-sim
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8001
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: llm-d-inference-sim
    app.kubernetes.io/instance: llm-d-inference-sim
---
# Source: llm-d-inference-sim/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-inference-sim
  labels:
    helm.sh/chart: llm-d-inference-sim-0.1.0
    app.kubernetes.io/name: llm-d-inference-sim
    app.kubernetes.io/instance: llm-d-inference-sim
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-d-inference-sim
      app.kubernetes.io/instance: llm-d-inference-sim
  template:
    metadata:
      annotations:
        checksum/config: 391624c79d2626126a0891e2e1d41eaf49c5801d38263692b2236846f481fa14
      labels:
        helm.sh/chart: llm-d-inference-sim-0.1.0
        app.kubernetes.io/name: llm-d-inference-sim
        app.kubernetes.io/instance: llm-d-inference-sim
        app.kubernetes.io/version: "v0.4.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - name: llm-d-inference-sim
          image: "ghcr.io/llm-d/llm-d-inference-sim:v0.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - --config
            - /config/config.yaml
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 8001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
          readinessProbe:
            httpGet:
              path: /ready
              port: http
          volumeMounts:
            - name: config
              mountPath: /config
      volumes:
        - name: config
          configMap:
            name: llm-d-inference-sim-config